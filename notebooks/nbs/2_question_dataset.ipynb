{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4938a0296d903c04",
   "metadata": {},
   "source": [
    "# Demo for MedAgent - Question Dataset Parsing\n",
    "This is the manual testing playground to test some basic workflows later properly implemented in the MedAgent repository.\n",
    "\n",
    "This file focuses on transforming the questions currently stored in an Excel sheet into something accessible via MongoDB. Additionally, the nature of the current dataset state is visualized."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## SETUP\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "from general.data_model.question_dataset import all_question_classes, all_supercategories\n",
    "from general.helper.mongodb_interactor import MongoDBInterface, CollectionName\n",
    "from scripts.QuestionDataset.question_setup import insert_question_classes\n",
    "from scripts.QuestionDataset.question_setup import insert_csv_entry_to_db\n",
    "from scripts.QuestionDataset.question_analysis import *\n",
    "\n",
    "mongo_url = os.getenv(\"MONGO_URL\", \"mongodb://mongo:mongo@host.docker.internal:27017/\")\n",
    "\n",
    "question_dataset_csv_location = \"input/question-dataset/question_dataset.csv\"\n",
    "statistics_doc = \"output/question_dataset/evaluation/statistics_document.txt\"\n",
    "questions_per_types, questions_per_gl = \"output/question_dataset/evaluation/questions_per_types.png\", \"output/question_dataset/evaluation/questions_per_gl.png\"\n",
    "\n",
    "for file_or_dir in [statistics_doc, questions_per_types, questions_per_gl]:\n",
    "    os.makedirs(os.path.dirname(file_or_dir), exist_ok=True)\n",
    "\n",
    "# Scale for screen display and saving options for all images\n",
    "screen_width, screen_height = 650, 450\n",
    "width, height = 750, 500\n",
    "\n",
    "dbi = MongoDBInterface(mongo_url)\n",
    "dbi.register_collections(\n",
    "    CollectionName.QUESTION_TYPES,\n",
    "    CollectionName.CORRECT_ANSWERS,\n",
    "    CollectionName.QUESTIONS,\n",
    "    CollectionName.GUIDELINES\n",
    ")"
   ],
   "id": "8f152c7a2179511f"
  },
  {
   "cell_type": "markdown",
   "id": "10b6ca4ef94a116",
   "metadata": {},
   "source": [
    "## Setup and insert question types"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "insert_question_classes(question_types_collection=dbi.get_collection(CollectionName.QUESTION_TYPES), classes=all_question_classes)",
   "id": "4d64f1d94d942b45"
  },
  {
   "cell_type": "markdown",
   "id": "752fdafb15fc3d98",
   "metadata": {},
   "source": [
    "## Load csv-file and transform to entries\n",
    "Important to mention here: the assumption is, that the csv content is valid. Meaning:\n",
    "- No bullshit categories (only existing ones)\n",
    "- Guideline exists and is OMS related\n",
    "- Answer text is in guideline on correct page; only negative examples can have empty answer"
   ]
  },
  {
   "cell_type": "code",
   "id": "d469a86edaa3e8bb",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df = pd.read_csv(question_dataset_csv_location, encoding=\"utf-8\", encoding_errors='ignore')\n",
    "\n",
    "counter_success, counter_failed = 0, 0\n",
    "for _, csv_entry in df.iterrows():\n",
    "    try:\n",
    "        insert_csv_entry_to_db(dbi=dbi, entry=csv_entry)\n",
    "        counter_success += 1\n",
    "    except Exception as e:\n",
    "        counter_failed += 1\n",
    "\n",
    "print(f\"Successful: {counter_success}, Failed: {counter_failed}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b25b5e03949eb137",
   "metadata": {},
   "source": [
    "## Analyze questions\n",
    "We will now look at some statistics for the created and translated question dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "7f8a7366db3fa115",
   "metadata": {},
   "source": [
    "total_number_questions = get_total_question_count(dbi.get_collection(CollectionName.QUESTIONS))\n",
    "\n",
    "df__question_dist, img__question_dist = analyze_and_visualize_question_distribution(\n",
    "    questions_collection=dbi.get_collection(CollectionName.QUESTIONS),\n",
    "    question_type_collection=dbi.get_collection(CollectionName.QUESTION_TYPES),\n",
    "    all_classes=all_question_classes,\n",
    "    supercategories=all_supercategories\n",
    ")\n",
    "df__quest_per_gl, img__quest_per_gl = analyze_and_visualize_question_per_guideline(\n",
    "    questions_coll=dbi.get_collection(CollectionName.QUESTIONS),\n",
    "    question_type_coll=dbi.get_collection(CollectionName.QUESTION_TYPES),\n",
    "    gl_coll=dbi.get_collection(CollectionName.GUIDELINES),\n",
    "    answer_coll=dbi.get_collection(CollectionName.CORRECT_ANSWERS)\n",
    ")\n",
    "\n",
    "number_questions = {\n",
    "    cat.value: int(df__question_dist.query(f\"supercategory == '{cat.value}' and subcategory.isna()\")[\"count\"].sum())\n",
    "    for cat in all_supercategories\n",
    "}\n",
    "\n",
    "percentages = {\n",
    "    cat: count / total_number_questions * 100\n",
    "    for cat, count in number_questions.items()\n",
    "}\n",
    "\n",
    "print(f\"Total number of questions: {total_number_questions}\")\n",
    "for supercat in all_supercategories:\n",
    "    print(\n",
    "        f\"Number of {supercat.value} questions: {number_questions[supercat.value]} (-> {percentages[supercat.value]:.2f}%)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5221f6873a67f20f",
   "metadata": {},
   "source": [
    "img__question_dist.update_layout(width=screen_width, height=screen_height)\n",
    "img__quest_per_gl.update_layout(width=screen_width, height=screen_height)\n",
    "\n",
    "img__question_dist.show()\n",
    "img__quest_per_gl.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "440b2cba96669eac",
   "metadata": {},
   "source": [
    "Can alternatively also save the images and numbers."
   ]
  },
  {
   "cell_type": "code",
   "id": "eed7e21132ee5c0b",
   "metadata": {},
   "source": [
    "with open(statistics_doc, \"w\", encoding=\"utf-8\") as statistics_file:\n",
    "    statistics_file.write(\n",
    "        f\"Total number of questions: {total_number_questions}\\n\" + \"\".join([\n",
    "            f\"Number of {supercat.value} questions: {number_questions[supercat.value]} (-> {percentages[supercat.value]:.2f}%)\\n\"\n",
    "            for supercat in all_supercategories\n",
    "        ])\n",
    "    )\n",
    "\n",
    "img__question_dist.write_image(questions_per_types, width=width, height=height)\n",
    "img__quest_per_gl.write_image(questions_per_gl, width=width, height=height)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a3fbbadb897a6de",
   "metadata": {},
   "source": [
    "## Shutdown"
   ]
  },
  {
   "cell_type": "code",
   "id": "b69ceb1367279593",
   "metadata": {},
   "source": [
    "dbi.close()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
