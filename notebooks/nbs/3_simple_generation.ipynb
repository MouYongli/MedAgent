{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4938a0296d903c04",
   "metadata": {},
   "source": [
    "# Demo for MedAgent - First answer generation with only generator\n",
    "This is the manual testing playground to test some basic workflows later properly implemented in the MedAgent repository.\n",
    "\n",
    "This file is responsible for a first test of answer generation (basically creating the first baseline for our system test). This means, answers are generated only with a generator. Further, a test for feedback gathering is established and the results analyzed and visualized."
   ]
  },
  {
   "cell_type": "code",
   "id": "3075e8d8e5c7b2c9",
   "metadata": {},
   "source": [
    "## SETUP\n",
    "import html\n",
    "import ipywidgets as widgets\n",
    "import markdown\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, clear_output\n",
    "from typing import List\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "from general.helper.logging import logger\n",
    "from general.helper.mongodb_interactor import MongoDBInterface, CollectionName\n",
    "from general.data_model.question_dataset import QuestionEntry, ExpectedAnswer, all_supercategories\n",
    "from general.data_model.system_interactions import WorkflowSystem, ChatInteraction, GenerationResultEntry, Feedback\n",
    "from scripts.System.system_setup import load_system_json\n",
    "from scripts.System.system_interaction import init_workflow, init_workflow_with_id, init_chat, pose_question, init_stored_wf_system, generate_stored_response\n",
    "from scripts.System.feedback_creation import create_feedback_correctness, create_feedback_hallucination_classification, insert_feedback\n",
    "from scripts.System.feedback_analysis import *\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.local-env\")\n",
    "\n",
    "mongo_url = os.getenv(\"MONGO_URL\", \"mongodb://mongo:mongo@host.docker.internal:27017/\")\n",
    "BACKEND_API_URL = \"http://host.docker.internal:5000/api\"\n",
    "\n",
    "azure_generator_only_config = load_system_json(\"./input/system/azure_generator_only.json\")\n",
    "statistics_doc = \"output/simple_generation/evaluation/statistics_document.txt\"\n",
    "response_time_img, correctness_score_img, hallucinations_img, accuracy_radar_img, accuracy_box_img = \"output/simple_generation/evaluation/response_time.png\", \"output/simple_generation/evaluation/correctness_scores.png\", \"output/simple_generation/evaluation/hallucinations_grid.png\", \"output/simple_generation/evaluation/accuracy_radar.png\", \"output/simple_generation/evaluation/accuracy_box.png\"\n",
    "\n",
    "screen_width, screen_height = 750, 500\n",
    "width, height = 750, 500\n",
    "\n",
    "dbi = MongoDBInterface(mongo_url)\n",
    "dbi.register_collections(\n",
    "    CollectionName.WORKFLOW_SYSTEMS,\n",
    "    CollectionName.CHAT_INTERACTIONS,\n",
    "    CollectionName.GENERATION_RESULT_ENTRIES,\n",
    "    CollectionName.QUESTIONS,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e59c5d3338dcb5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test out answer submission"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3b7d6ffa372a972",
   "metadata": {},
   "source": [
    "azure_generator_only_wf = dbi.get_entry(CollectionName.WORKFLOW_SYSTEMS, \"name\", azure_generator_only_config[\"name\"])\n",
    "if azure_generator_only_wf is None:\n",
    "    azure_generator_only_wf_id = init_workflow(BACKEND_API_URL, azure_generator_only_config)\n",
    "else:\n",
    "    azure_generator_only_wf_id = dbi.document_to_workflow_system(azure_generator_only_wf).workflow_id\n",
    "    azure_generator_only_wf_id = init_workflow_with_id(BACKEND_API_URL, azure_generator_only_config, azure_generator_only_wf_id)\n",
    "\n",
    "azure_generator_only_chat = init_chat(BACKEND_API_URL, azure_generator_only_wf_id)\n",
    "question = dbi.get_collection(CollectionName.QUESTIONS).find_one().get(\"question\")\n",
    "answer, retrieval, response_latency = pose_question(BACKEND_API_URL, azure_generator_only_chat, question)\n",
    "\n",
    "print(f\"### QUESTION: ###\\n{question}\")\n",
    "print(f\"--------------------------------------------------\")\n",
    "print(f\"### ANSWER in {response_latency:.2f} seconds: ###\\n{answer}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e12d046353a42583",
   "metadata": {},
   "source": [
    "## Creating answers\n",
    "Next up, we will create all responses for our currently stored questions and store them. This will be done at first only for one configuration (azure generator only), but can be copied later on for further generations."
   ]
  },
  {
   "cell_type": "code",
   "id": "91e65ba48b961fe5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "azure_generator_only_wf_system: WorkflowSystem = init_stored_wf_system(dbi, azure_generator_only_config, BACKEND_API_URL)   \n",
    "\n",
    "all_question_docs = list(dbi.get_collection(CollectionName.QUESTIONS).find())\n",
    "logger.progress(\"Progress on answer generation [PROGRESS]: \", 0, len(all_question_docs))\n",
    "processed_ids = [x for x in range(0, 60)]\n",
    "for i, question_doc in enumerate(all_question_docs):\n",
    "    if i in processed_ids:\n",
    "        logger.progress(\"Progress on answer generation [PROGRESS]:\", i + 1, len(all_question_docs))    \n",
    "        continue\n",
    "    question: QuestionEntry = dbi.document_to_question_entry(question_doc)\n",
    "    generate_stored_response(dbi, azure_generator_only_wf_system, None, question, BACKEND_API_URL)\n",
    "    logger.progress(\"Progress on answer generation [PROGRESS]:\", i + 1, len(all_question_docs))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da43d45d2edbb428",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gathering feedback\n",
    "For this configuration, we now want to obtain not only the response latency, but also two manual evaluation values:\n",
    "1. Correctness\n",
    "2. Hallucination classification\n",
    "\n",
    "This requires expert responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c61c35d602fc9db",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df287b44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Get chats"
   ]
  },
  {
   "cell_type": "code",
   "id": "f93a3bca",
   "metadata": {},
   "source": [
    "def get_chats():\n",
    "    azure_generator_only_wf_system: WorkflowSystem = dbi.document_to_workflow_system(\n",
    "        dbi.get_entry(CollectionName.WORKFLOW_SYSTEMS, \"name\", azure_generator_only_config[\"name\"])\n",
    "    )\n",
    "    chats: List[ChatInteraction] = azure_generator_only_wf_system.generation_results\n",
    "    \n",
    "    return chats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f12f4013",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Define functions for widget creation"
   ]
  },
  {
   "cell_type": "code",
   "id": "7118961ce33f603b",
   "metadata": {},
   "source": [
    "def create_ui_widgets():\n",
    "  title_html = widgets.HTML()\n",
    "  question_html = widgets.HTML(layout=widgets.Layout(width=\"75%\"))\n",
    "  expected_asw_html = widgets.HTML(layout=widgets.Layout(width=\"75%\"))\n",
    "  provided_asw_html = widgets.HTML(layout=widgets.Layout(width=\"75%\"))\n",
    "  existing_feedback_html = widgets.HTML(layout=widgets.Layout(width=\"75%\"))\n",
    "  \n",
    "  feedback_correct_label = widgets.Label(value=\"Correctness:\", layout=widgets.Layout(width='120px'))\n",
    "  feedback_correct_input = widgets.IntSlider(value=3, min=1, max=5, step=1, description=\"\")\n",
    "  \n",
    "  feedback_hall_label = widgets.Label(value=\"Hallucination count:\", layout=widgets.Layout(width='120px'))\n",
    "  fc_input = widgets.BoundedIntText(value=0, min=0, max=50, description=\"FC\")\n",
    "  ic_input = widgets.BoundedIntText(value=0, min=0, max=50, description=\"IC\")\n",
    "  cc_input = widgets.BoundedIntText(value=0, min=0, max=50, description=\"CC\")\n",
    "  \n",
    "  feedback_notes_label = widgets.Label(value=\"Note:\", layout=widgets.Layout(width='120px'))\n",
    "  feedback_notes_input = widgets.Textarea(placeholder=\"Enter your name and optional notes...\", description=\"\", disabled=False, layout=widgets.Layout(width=\"75%\", padding=\"0 160px 0 0\"))  \n",
    "  \n",
    "  return title_html, question_html, expected_asw_html, provided_asw_html, existing_feedback_html, feedback_correct_label, feedback_correct_input, feedback_hall_label, fc_input, ic_input, cc_input, feedback_notes_label, feedback_notes_input\n",
    "\n",
    "def create_buttons():\n",
    "  prev_button   = widgets.Button(description=\"Previous\")\n",
    "  next_button   = widgets.Button(description=\"Next\")\n",
    "  save_button   = widgets.Button(description=\"Save\", button_style=\"success\")\n",
    "  exit_button = widgets.Button(description=\"Exit\")\n",
    "  \n",
    "  return prev_button, next_button, save_button, exit_button"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0cacd8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Define parsers for display of chat properties"
   ]
  },
  {
   "cell_type": "code",
   "id": "b3c37e0f",
   "metadata": {},
   "source": [
    "def parse_question(question: QuestionEntry, section_title=\"Question\") -> str:\n",
    "  question = f\"\"\"\n",
    "  <details open style=\"margin: 0 20px\">\n",
    "    <summary style='font-weight: bold; font-size: 1.05em; cursor: pointer;'>{section_title}</summary>\n",
    "    <div style='margin: 0px 30px 30px 30px;'>\n",
    "      <span>{html.escape(question.question)}</span><br>\n",
    "      <span><small style='color: gray'>Type: {question.classification.supercategory.value} / {question.classification.subcategory.value}</small></span>\n",
    "    </div>\n",
    "  </details>\n",
    "  \"\"\"\n",
    "  return question\n",
    "\n",
    "def parse_expected_answer_table(ea_list: List[ExpectedAnswer], section_title = \"Expected answer / retrieval\") -> str:\n",
    "  align_style = \"text-align:left; padding: 4px 10px;\"\n",
    "  table = f\"\"\"\n",
    "  <details open style=\"margin: 0 20px\">\n",
    "    <summary style='font-weight: bold; font-size: 1.05em; cursor: pointer;'>{section_title}</summary>\n",
    "    <div style='margin: 0px 30px 30px 30px;'>\n",
    "      <table style='border-collapse: collapse;'>\n",
    "        <thead>\n",
    "          <tr>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>#</th>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>Guideline (AWMF-Nr.)</th>\n",
    "            <th style='{align_style}; border-bottom: 1px solid #ccc;'>Text</th>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>Page in guideline</th>\n",
    "          </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "  \"\"\"\n",
    "  for i, ea in enumerate(ea_list):\n",
    "    table += f\"\"\"\n",
    "          <tr>\n",
    "            <td style='{align_style}'>{i}</td>\n",
    "            <td style='{align_style}'>{ea.guideline.awmf_register_number}</td>\n",
    "            <td style='{align_style}'>{html.escape(ea.text)}</td>\n",
    "            <td style='{align_style}'>{ea.guideline_page}</td>\n",
    "          </tr>\n",
    "    \"\"\"\n",
    "  table += \"\"\"\n",
    "        </tbody>\n",
    "      </table>\n",
    "    </div>\n",
    "  </details>\n",
    "  \"\"\"\n",
    "  return table\n",
    "\n",
    "def parse_provided_answer(pa: GenerationResultEntry, section_title=\"System response (provided answer)\") -> str:\n",
    "  provided_answer = f\"\"\"\n",
    "  <details open style=\"margin: 0 20px\">\n",
    "    <summary style='font-weight: bold; font-size: 1.05em; cursor: pointer;'>{section_title}</summary>\n",
    "    <div style='margin: 0px 30px 30px 30px;'>\n",
    "      {markdown.markdown(pa.answer)}\n",
    "    </div>\n",
    "  </details>\n",
    "  \"\"\"\n",
    "  return provided_answer\n",
    "\n",
    "def parse_existing_feedback(fb_list: List[Feedback], section_title=\"Already existing evaluation / feedback\") -> str:\n",
    "  if not fb_list:\n",
    "    return \"\"\n",
    "  \n",
    "  align_style = \"text-align:left; padding: 4px 10px;\"\n",
    "  table = f\"\"\"\n",
    "  <details open style=\"margin: 0 20px\">\n",
    "    <summary style='font-weight: bold; font-size: 1.05em; cursor: pointer;'>{section_title}</summary>\n",
    "    <div style='margin: 0px 30px 30px 30px;'>\n",
    "      <table style='border-collapse: collapse;'>\n",
    "        <thead>\n",
    "          <tr>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>#</th>\n",
    "            <th style='{align_style}; border-bottom: 1px solid #ccc;'>Target</th>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>Type</th>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>Value</th>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>Notes</th>\n",
    "          </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "  \"\"\"\n",
    "  for i, fb in enumerate(fb_list):\n",
    "    table += f\"\"\"\n",
    "          <tr>\n",
    "            <td style='{align_style}'>{i}</td>\n",
    "            <td style='{align_style}'>{fb.target.value}</td>\n",
    "            <td style='{align_style}'>{fb.type.value}</td>\n",
    "            <td style='{align_style}'>{fb.value} <small style='color: gray;'>{'manual' if fb.manual else 'autom.'}</small></td>\n",
    "            <td style='{align_style}'>{fb.notes if fb.notes else '/'}</td>\n",
    "          </tr>\n",
    "    \"\"\"\n",
    "  table += \"\"\"\n",
    "        </tbody>\n",
    "      </table>\n",
    "    </div>\n",
    "  </details>\n",
    "  \"\"\"\n",
    "  return table"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "668d2c89a1410f5f",
   "metadata": {},
   "source": [
    "#### Actual setup of input display"
   ]
  },
  {
   "cell_type": "code",
   "id": "af323d7a731e871e",
   "metadata": {},
   "source": [
    "current_idx = 0\n",
    "\n",
    "title_html, question_html, expected_answer_html, provided_answer_html, existing_feedback_html, feedback_correct_label, feedback_correct_input, feedback_hall_label, fc_input, ic_input, cc_input, feedback_notes_label, feedback_notes_input = create_ui_widgets()\n",
    "prev_button, next_button, save_button, exit_button = create_buttons()\n",
    "\n",
    "def update(dbi, chat: ChatInteraction, entry: GenerationResultEntry):\n",
    "  correctness_score = feedback_correct_input.value\n",
    "  hallucination_counts = {\n",
    "    \"FC\": fc_input.value,\n",
    "    \"IC\": ic_input.value,\n",
    "    \"CC\": cc_input.value\n",
    "  }\n",
    "  note = feedback_notes_input.value\n",
    "  \n",
    "  insert_feedback(\n",
    "    dbi, chat, entry, create_feedback_correctness(correctness_score, note)\n",
    "  )\n",
    "  insert_feedback(\n",
    "    dbi, chat, entry, create_feedback_hallucination_classification(hallucination_counts, note)\n",
    "  )\n",
    "\n",
    "def show_entry(idx):\n",
    "  global current_idx\n",
    "  current_idx = idx\n",
    "  entry: GenerationResultEntry = chats[idx].entries[0]\n",
    "  \n",
    "  title_html.value = f\"\"\"\n",
    "  <div style='display: flex; justify-content: start; align-items: center;'>\n",
    "    <h3 style='margin: 0;'>Chat {idx}</h3>\n",
    "    <small style='margin-left: 10px;'>[{idx + 1}/{len(chats)}]</small>\n",
    "  </div>\n",
    "  \"\"\"\n",
    "  question_html.value = parse_question(entry.question)\n",
    "  expected_answer_html.value = parse_expected_answer_table(entry.question.expected_answers)\n",
    "  provided_answer_html.value = parse_provided_answer(entry)\n",
    "  existing_feedback_html.value = parse_existing_feedback(entry.feedback)\n",
    "  \n",
    "  prev_button.disabled = (idx == 0)\n",
    "  next_button.disabled = (idx == len(chats) - 1)\n",
    "  save_button.disabled = (idx == len(chats))\n",
    "\n",
    "def on_prev_clicked(b):\n",
    "  \"\"\"Handle Previous button click: NO SAVING!! show previous entry.\"\"\"\n",
    "  if current_idx > 0:\n",
    "    show_entry(current_idx - 1)\n",
    "\n",
    "def on_next_clicked(b):\n",
    "  \"\"\"Handle Next button click: NO SAVING!! show next entry.\"\"\"\n",
    "  if current_idx < len(chats) - 1:\n",
    "    show_entry(current_idx + 1)\n",
    "\n",
    "def on_save_clicked(b):\n",
    "  \"\"\"Handle Save button click: store current status.\"\"\"\n",
    "  global chats\n",
    "  current_entry = chats[current_idx]\n",
    "  update(dbi, current_entry, current_entry.entries[0])\n",
    "  chats = get_chats()\n",
    "  show_entry(current_idx)\n",
    "  print(f\"stored results for entry {current_idx}\")\n",
    "\n",
    "def on_exit_clicked(b):\n",
    "  \"\"\"Handle Exit button click: NO SAVING!! end the process.\"\"\"\n",
    "  entry_box.layout.display = 'none'\n",
    "  clear_output()\n",
    "  print(\"Finished reviewing entries.\") \n",
    "\n",
    "prev_button.on_click(on_prev_clicked)\n",
    "next_button.on_click(on_next_clicked)\n",
    "save_button.on_click(on_save_clicked)\n",
    "exit_button.on_click(on_exit_clicked)\n",
    "\n",
    "entry_box = widgets.VBox([\n",
    "  title_html,\n",
    "  question_html,\n",
    "  expected_answer_html,\n",
    "  provided_answer_html,\n",
    "  existing_feedback_html,\n",
    "  widgets.HBox([feedback_correct_label,  feedback_correct_input], layout=widgets.Layout(margin=\"10px 20px\")),  \n",
    "  widgets.HBox([feedback_hall_label, widgets.HBox([fc_input, ic_input, cc_input])], layout=widgets.Layout(margin=\"10px 20px\")), \n",
    "  widgets.HBox([feedback_notes_label,  feedback_notes_input], layout=widgets.Layout(margin=\"10px 20px\")),  \n",
    "  widgets.HBox([prev_button, next_button, save_button, exit_button], layout=widgets.Layout(margin=\"5px 20px\"))\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78ba1a5cd55b3190",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This section requires expert input. Start the next cell, then fill out the provided form.\n",
    "\n",
    "We assume for each chat, that only one interaction is relevant. Meaning, we got one question as an input, and one system response as the output. This means, per question, we got the following information displayed:\n",
    "- Expected answer / retrieval (if available).\n",
    "- Answer provided by the system &rarr; needs evaluation.\n",
    "- Display of current evaluation.\n",
    "\n",
    "The evaluation is focussing on the following aspects:\n",
    "1. Correctness: Check how well response covers the expected information (1 - <small>$0\\%$</small>, 5 - <small>$100\\%$</small>)\n",
    "2. Hallucination classification: Identify and categorize hallucinations &rarr; count for each of these categories:\n",
    "   - FC = fact-conflicting (contradicts world (medical) knowledge)\n",
    "   - IC = input-conflicting (does not address full user input)\n",
    "   - CC = context-conflicting (contradicts provided content like user history or citations)\n",
    "\n",
    "In addition to the values, you can also add a comment per feedback submission. Ideally, add the following line:\n",
    "- `Evaluator name: ...` with your name inserted\n",
    "\n",
    "For the navigation, keep the following in mind:\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90a6e3809a0ef1",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "id": "74ff65f59649f195",
   "metadata": {},
   "source": [
    "chats: List[ChatInteraction] = get_chats()\n",
    "show_entry(0)\n",
    "entry_box.layout.display = \"\"\n",
    "display(entry_box)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43a353aa",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "This section will now present an overview over the collected feedback and statistics for the evaluation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c985a945",
   "metadata": {},
   "source": [
    "### Response latency"
   ]
  },
  {
   "cell_type": "code",
   "id": "f23696b3",
   "metadata": {},
   "source": [
    "azure_generator_only_wf_system: WorkflowSystem = init_stored_wf_system(dbi, azure_generator_only_config, BACKEND_API_URL)\n",
    "avg_response_time = get_average_response_time(azure_generator_only_wf_system)\n",
    "df__response_time, img__response_time = analyze_and_visualize_response_time_per_category(azure_generator_only_wf_system)\n",
    "img__response_time.update_layout(width=screen_width, height=screen_height)\n",
    "\n",
    "print(f\"Average response latency: {avg_response_time:.2f} s\")\n",
    "for supercat in all_supercategories:\n",
    "    entry = df__response_time[df__response_time['subcategory'].isna() & (df__response_time['supercategory'] == supercat.value)]['avg_response_latency'].iloc[0]\n",
    "    if pd.isna(entry):\n",
    "        print(f\"No response latency for {supercat.value} questions\")\n",
    "    else:\n",
    "        print(f\"Response latency for {supercat.value} questions: {entry:.2f} s\")\n",
    "        \n",
    "img__response_time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b15edfd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Correctness of overall system"
   ]
  },
  {
   "cell_type": "code",
   "id": "e5cd12ca8afdff3f",
   "metadata": {},
   "source": [
    "azure_generator_only_wf_system: WorkflowSystem = init_stored_wf_system(dbi, azure_generator_only_config, BACKEND_API_URL)\n",
    "avg_correctness = get_average_correctness(azure_generator_only_wf_system)\n",
    "df__correctness, img__correctness = analyze_and_visualize_correctness_per_category(azure_generator_only_wf_system)\n",
    "img__correctness.update_layout(width=screen_width, height=screen_height)\n",
    "\n",
    "\n",
    "print(f\"Average correctness: {avg_correctness:.2f}\")\n",
    "for supercat in all_supercategories:\n",
    "    entry = df__correctness[df__correctness['subcategory'].isna() & (df__correctness['supercategory'] == supercat.value)]['avg_correctness_score'].iloc[0]\n",
    "    if pd.isna(entry):\n",
    "        print(f\"No correctness scores for {supercat.value} questions\")\n",
    "    else:\n",
    "        print(f\"Correctness score for {supercat.value} questions: {entry:.2f}\")\n",
    "        \n",
    "img__correctness"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9344172f6c1020e",
   "metadata": {},
   "source": [
    "azure_generator_only_wf_system: WorkflowSystem = init_stored_wf_system(dbi, azure_generator_only_config, BACKEND_API_URL)\n",
    "list_hallucinations = get_sum_hallucinations_per_question(azure_generator_only_wf_system)\n",
    "avg_count_hallucinations_per_question = sum(list_hallucinations) / sum(1 for item in list_hallucinations if item is not None)\n",
    "df__hallucinations, img__hallucinations = analyze_and_visualize_hallucinations(azure_generator_only_wf_system)\n",
    "img__hallucinations.update_layout(width=2*screen_width, height=screen_height)\n",
    "\n",
    "print(f\"Overall hallucinations per question: {avg_count_hallucinations_per_question:.2f}\")\n",
    "\n",
    "img__hallucinations"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eaeb91f051f1fcda",
   "metadata": {},
   "source": [
    "### \"Accuracy\" of system\n",
    "Compare provided answer and expected retrieval with ROUGE, BLEU, METEOR, and embedding similarity."
   ]
  },
  {
   "cell_type": "code",
   "id": "4c64cbee5de53d26",
   "metadata": {},
   "source": [
    "azure_generator_only_wf_system: WorkflowSystem = init_stored_wf_system(dbi, azure_generator_only_config, BACKEND_API_URL)\n",
    "avg_rouge_1, avg_rouge_l, avg_bleu, avg_meteor, avg_similarity = get_average_accuracy_scores(azure_generator_only_wf_system)\n",
    "df__accuracy, img__accuracy_radar, img__accuracy_box = analyze_and_visualize_accuracy_per_category(azure_generator_only_wf_system)\n",
    "img__accuracy_radar.update_layout(width=screen_width, height=screen_height)\n",
    "img__accuracy_box.update_layout(width=2.5*screen_width, height=screen_height)\n",
    "\n",
    "print(f\"Average ROUGE-1 [{avg_rouge_1:.2f}], ROUGE-L [{avg_rouge_l:.2f}], BLEU [{avg_bleu:.2f}], METEOR [{avg_meteor:.2f}], embedding similarity [{avg_similarity:.2f}]\")\n",
    "for supercat in all_supercategories:\n",
    "    r1_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_rouge_1_score'].iloc[0]\n",
    "    rl_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_rouge_l_score'].iloc[0]\n",
    "    b_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_bleu_score'].iloc[0]\n",
    "    m_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_meteor_score'].iloc[0]\n",
    "    s_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_embedding_sim_score'].iloc[0]\n",
    "\n",
    "    r1_print = f\"{r1_entry:.2f}\" if not pd.isna(r1_entry) else \"N/A\"\n",
    "    rl_print = f\"{rl_entry:.2f}\" if not pd.isna(rl_entry) else \"N/A\"\n",
    "    b_print = f\"{b_entry:.2f}\" if not pd.isna(b_entry) else \"N/A\"\n",
    "    m_print = f\"{m_entry:.2f}\" if not pd.isna(m_entry) else \"N/A\"\n",
    "    s_print = f\"{s_entry:.2f}\" if not pd.isna(s_entry) else \"N/A\"\n",
    "    print(f\"-> For {supercat.value}: ROUGE-1 [{r1_print}], ROUGE-L [{rl_print}], BLEU [{b_print}], METEOR [{m_print}], embedding similarity [{s_print}]\")\n",
    "\n",
    "img__accuracy_radar.show()\n",
    "img__accuracy_box.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3ad1e4e",
   "metadata": {},
   "source": [
    "### Storing\n",
    "Can alternatively also save the images and numbers."
   ]
  },
  {
   "cell_type": "code",
   "id": "e7151677622a6fbb",
   "metadata": {},
   "source": [
    "os.makedirs(os.path.dirname(statistics_doc), exist_ok=True)\n",
    "with open(statistics_doc, \"a\", encoding=\"utf-8\") as statistics_file:\n",
    "    # Response latency\n",
    "    statistics_file.write(f\"\\n\\nAverage response latency across all questions: {avg_response_time:.2f} s\\n\")\n",
    "    for supercat in all_supercategories:\n",
    "        entry = df__response_time[\n",
    "            df__response_time['subcategory'].isna() &\n",
    "            (df__response_time['supercategory'] == supercat.value)\n",
    "        ]['avg_response_latency']\n",
    "        if len(entry) == 0:\n",
    "            statistics_file.write(f\"No response latency for {supercat.value} questions\\n\")\n",
    "        else:\n",
    "            statistics_file.write(f\"Response latency for {supercat.value} questions: {entry.iloc[0]:.2f} s\\n\")\n",
    "    \n",
    "img__response_time.write_image(response_time_img, width=width, height=height)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3eb517c7f563fa1",
   "metadata": {},
   "source": [
    "os.makedirs(os.path.dirname(statistics_doc), exist_ok=True)\n",
    "with open(statistics_doc, \"a\", encoding=\"utf-8\") as statistics_file:\n",
    "    # Correctness score\n",
    "    statistics_file.write(f\"\\n\\nAverage correctness score across all questions: {avg_correctness:.2f}\\n\")\n",
    "    for supercat in all_supercategories:\n",
    "        entry = df__correctness[\n",
    "            df__correctness['subcategory'].isna() &\n",
    "            (df__correctness['supercategory'] == supercat.value)\n",
    "        ]['avg_correctness_score']\n",
    "        if entry.empty or pd.isna(entry.iloc[0]):\n",
    "            statistics_file.write(f\"No correctness scores for {supercat.value} questions\\n\")\n",
    "        else:\n",
    "            statistics_file.write(f\"Correctness score for {supercat.value} questions: {entry.iloc[0]:.2f}\\n\")\n",
    "\n",
    "img__correctness.write_image(correctness_score_img, width=width, height=height)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29f799158f274db8",
   "metadata": {},
   "source": [
    "os.makedirs(os.path.dirname(statistics_doc), exist_ok=True)\n",
    "with open(statistics_doc, \"a\", encoding=\"utf-8\") as statistics_file:\n",
    "    # Hallucinations\n",
    "    statistics_file.write(f\"\\n\\nAverage hallucination count per question: {avg_count_hallucinations_per_question:.2f}\\n\")\n",
    "\n",
    "img__hallucinations.write_image(hallucinations_img, width=2.5 * width, height=height)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ba93ab1-6392-489c-9344-a73173ce1a09",
   "metadata": {},
   "source": [
    "os.makedirs(os.path.dirname(statistics_doc), exist_ok=True)\n",
    "with open(statistics_doc, \"a\", encoding=\"utf-8\") as statistics_file:\n",
    "    # \"Accuracy\"\n",
    "    statistics_file.write(f\"\\n\\nAverage ROUGE-1 [{avg_rouge_1:.2f}], ROUGE-L [{avg_rouge_l:.2f}], BLEU [{avg_bleu:.2f}], METEOR [{avg_meteor:.2f}], embedding similarity [{avg_similarity:.2f}]\\n\")\n",
    "    for supercat in all_supercategories:\n",
    "        r1_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_rouge_1_score'].iloc[0]\n",
    "        rl_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_rouge_l_score'].iloc[0]\n",
    "        b_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_bleu_score'].iloc[0]\n",
    "        m_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_meteor_score'].iloc[0]\n",
    "        s_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_embedding_sim_score'].iloc[0]\n",
    "\n",
    "        r1_print = f\"{r1_entry:.2f}\" if not pd.isna(r1_entry) else \"N/A\"\n",
    "        rl_print = f\"{rl_entry:.2f}\" if not pd.isna(rl_entry) else \"N/A\"\n",
    "        b_print = f\"{b_entry:.2f}\" if not pd.isna(b_entry) else \"N/A\"\n",
    "        m_print = f\"{m_entry:.2f}\" if not pd.isna(m_entry) else \"N/A\"\n",
    "        s_print = f\"{s_entry:.2f}\" if not pd.isna(s_entry) else \"N/A\"\n",
    "        statistics_file.write(f\"-> For {supercat.value}: ROUGE-1 [{r1_print}], ROUGE-L [{rl_print}], BLEU [{b_print}], METEOR [{m_print}], embedding similarity [{s_print}]\\n\")\n",
    "    \n",
    "img__accuracy_radar.write_image(accuracy_radar_img, width=width, height=height)\n",
    "img__accuracy_box.write_image(accuracy_box_img, width=2.5*width, height=height)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f8202e0576fd894",
   "metadata": {},
   "source": [
    "## Shutdown"
   ]
  },
  {
   "cell_type": "code",
   "id": "fc0cf44b46f64f56",
   "metadata": {},
   "source": [
    "dbi.close()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
