{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60525002ddf614db",
   "metadata": {},
   "source": [
    "# Demo for MedAgent - First answer generation with naive RAG pipeline\n",
    "\n",
    "This is the manual testing playground to test some basic workflows later properly implemented in the MedAgent repository.\n",
    "\n",
    "This file is responsible for a first test of answer generation with naive retrieval (basically creating the second baseline for our system test). This means, for the question first the most similar chunks from the guidelines are retrieved, and then provided to a generator with the original question. For this setup, new feedback must be gathered and the results analyzed and visualized."
   ]
  },
  {
   "cell_type": "code",
   "id": "45808d215d25f669",
   "metadata": {},
   "source": [
    "# SETUP\n",
    "import html\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import markdown\n",
    "import tiktoken\n",
    "from IPython.core.display_functions import clear_output\n",
    "from dotenv import load_dotenv\n",
    "from ipywidgets import widgets\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "from general.data_model.question_dataset import ExpectedAnswer\n",
    "from general.data_model.guideline_metadata import GuidelineMetadata\n",
    "from scripts.Guideline.guideline_interaction import get_plain_text_from_pdf\n",
    "from scripts.System.system_setup import load_system_json\n",
    "from scripts.System.system_interaction import *\n",
    "from scripts.System.feedback_creation import *\n",
    "from scripts.System.feedback_analysis import *\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.local-env\")\n",
    "BACKEND_API_URL = \"http://host.docker.internal:5000/api\"\n",
    "mongo_url = os.getenv(\"MONGO_URL\", \"mongodb://mongo:mongo@host.docker.internal:27017/\")\n",
    "\n",
    "weaviate_db_config = load_system_json(\"./input/database_setups/weaviate_custom_vectorizer.json\")\n",
    "naive_rag_azure_config = load_system_json(\"./input/system/naive_rag_azure.json\")\n",
    "inserted_guidelines = load_system_json(\"./output/naive_rag/chunk_indexing.json\")\n",
    "text_output_dir = \"output/guideline/plain_text/\"\n",
    "statistics_doc = \"output/naive_rag/evaluation/statistics_document.txt\"\n",
    "response_time_img, correctness_score_img, hallucinations_img, accuracy_radar_img, accuracy_box_img, retrieval_precision_img, retrieval_recall_img, retrieval_f1_img, retrieval_factuality_img = \"output/naive_rag/evaluation/response_time.png\", \"output/naive_rag/evaluation/correctness_scores.png\", \"output/naive_rag/evaluation/hallucinations_grid.png\", \"output/naive_rag/evaluation/accuracy_radar.png\", \"output/naive_rag/evaluation/accuracy_box.png\", \"output/naive_rag/evaluation/retrieval_precision.png\", \"output/naive_rag/evaluation/retrieval_recall.png\", \"output/naive_rag/evaluation/retrieval_f1.png\", \"output/naive_rag/evaluation/factuality.png\"\n",
    "\n",
    "for file_or_dir in [text_output_dir, statistics_doc]:\n",
    "    os.makedirs(os.path.dirname(file_or_dir), exist_ok=True)\n",
    "\n",
    "screen_width, screen_height = 750, 500\n",
    "width, height = 750, 500\n",
    "\n",
    "dbi = MongoDBInterface(mongo_url)\n",
    "dbi.register_collections(\n",
    "    CollectionName.GUIDELINES,\n",
    "    CollectionName.WORKFLOW_SYSTEMS,\n",
    "    CollectionName.QUESTIONS\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd5e9e53491be494",
   "metadata": {},
   "source": [
    "## Setup vector database\n",
    "In the first jupyter notebook, the guideline were already downloaded and stored in a MongoDB. To now be utilizable for the naive RAG flow, their content now needs to be cut up and stored in a vector database (for now Milvus with chunk size of 512)."
   ]
  },
  {
   "cell_type": "code",
   "id": "2cc3dd3a-592f-4055-b947-e2207be757af",
   "metadata": {},
   "source": [
    "guideline_documents = list(dbi.get_collection(CollectionName.GUIDELINES).find())\n",
    "guidelines = [\n",
    "    dbi.document_to_guideline_metadata(doc) for doc in guideline_documents\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9af2ad42-fc5f-4dfc-aef4-06ee9a1b04ee",
   "metadata": {},
   "source": [
    "# comment out if not want to overwrite\n",
    "response = requests.delete(f\"{BACKEND_API_URL}/knowledge/vector/retriever/delete/{weaviate_db_config['class_name']}\")\n",
    "logger.info(f\"Result of deletion for {weaviate_db_config['class_name']}: {response}\")\n",
    "\n",
    "response = requests.post(f\"{BACKEND_API_URL}/knowledge/vector/retriever/init\", json=weaviate_db_config)\n",
    "try:\n",
    "    response.raise_for_status()\n",
    "    logger.info(response)\n",
    "except Exception as e:\n",
    "    detail = response.json().get(\"detail\", \"\")\n",
    "    if \"already exists\" in detail:\n",
    "        logger.info(f\"Weaviate collection already exists: {detail}\")\n",
    "    else:\n",
    "        logger.error(f\"Failed to initialize Weaviate collection: {detail}\")\n",
    "        raise"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "76f4146a859e6c04",
   "metadata": {},
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 128) -> List[str]:\n",
    "    words = text.split()\n",
    "    chunks, current = [], []\n",
    "    token_count = lambda x: len(encoding.encode(\" \".join(x)))\n",
    "\n",
    "    for word in words:\n",
    "        current.append(word)\n",
    "        if token_count(current) >= max_tokens:\n",
    "            chunks.append(\" \".join(current[:-1]))\n",
    "            current = [word]\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66f8fedc-b413-45ba-a362-b1d0a6ef1f26",
   "metadata": {},
   "source": [
    "def insert_for_guideline(guideline: GuidelineMetadata):\n",
    "    logger.info(\n",
    "        f\"Processing guideline {guideline.awmf_register_number} ({guideline.download_information.page_count} pages)\")\n",
    "    text = get_plain_text_from_pdf(guideline.download_information.file_path, text_output_dir)\n",
    "    chunks = chunk_text(text)\n",
    "    if chunks == []:\n",
    "        logger.error(f\"[{g.awmf_register_numner}] Something went wrong with reading the text or chunking -> empty\")\n",
    "    logger.progress(f\"Processing guideline {guideline.awmf_register_number} [PROGRESS]: \", 0, len(chunks))\n",
    "    non_successful_chunks = []\n",
    "    for i_c, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            #vector = embedder.embed(chunk)\n",
    "            insert_entity = {\n",
    "                \"text\": chunk,\n",
    "                #\"vector\": vector,\n",
    "                \"metadata\": {\n",
    "                    \"guideline_id\": guideline.awmf_register_number,\n",
    "                    \"chunk_index\": i_c\n",
    "                },\n",
    "                \"class_name\": weaviate_db_config['class_name']\n",
    "            }\n",
    "            #logger.info(insert_entity)\n",
    "            response = requests.post(\n",
    "                f\"{BACKEND_API_URL}/knowledge/vector/retriever/insert\",\n",
    "                json=insert_entity\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "        except Exception as chunk_error:\n",
    "            logger.error(f\"[{g.awmf_register_number}] Failed to process chunk {i_c}: {chunk_error}\")\n",
    "            non_successful_chunks.append({i_c: chunk})\n",
    "\n",
    "        logger.progress(f\"Processing guideline {guideline.awmf_register_number} [PROGRESS]:\", i_c + 1, len(chunks))\n",
    "\n",
    "    if non_successful_chunks != []:\n",
    "        logger.error(f\"Problems with inserting these chunks: {non_successful_chunks}\")\n",
    "    else:\n",
    "        logger.success(f\"Successfully transferred whole guideline with {len(chunks)} chunks\")\n",
    "\n",
    "\n",
    "def insert_batch_for_guideline(guideline: GuidelineMetadata):\n",
    "    logger.info(\n",
    "        f\"Processing guideline {guideline.awmf_register_number} ({guideline.download_information.page_count} pages)\")\n",
    "    text = get_plain_text_from_pdf(guideline.download_information.file_path, text_output_dir)\n",
    "    chunks = chunk_text(text)\n",
    "    if chunks == []:\n",
    "        logger.error(f\"[{g.awmf_register_numner}] Something went wrong with reading the text or chunking -> empty\")\n",
    "    logger.progress(f\"Tranforming chunks {guideline.awmf_register_number} [PROGRESS]: \", 0, len(chunks))\n",
    "\n",
    "    batch_entities = []\n",
    "    for i_c, chunk in enumerate(chunks):\n",
    "        #vector = embedder.embed(chunk)\n",
    "        insert_entity = {\n",
    "            \"text\": chunk,\n",
    "            #\"vector\": vector,\n",
    "            \"metadata\": {\n",
    "                \"guideline_id\": guideline.awmf_register_number,\n",
    "                \"chunk_index\": i_c\n",
    "            },\n",
    "            \"class_name\": weaviate_db_config['class_name']\n",
    "        }\n",
    "        batch_entities.append(insert_entity)\n",
    "        logger.progress(f\"Tranforming chunks {guideline.awmf_register_number} [PROGRESS]: \", i_c + 1, len(chunks))\n",
    "\n",
    "    logger.info(f\"Submitting batch upload\")\n",
    "    response = requests.post(\n",
    "        f\"{BACKEND_API_URL}/knowledge/vector/retriever/insertBatch\",\n",
    "        json={\n",
    "            \"class_name\": weaviate_db_config['class_name'],\n",
    "            \"entries\": batch_entities\n",
    "        }\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    logger.info(f\"Response: {response.json()}\")\n",
    "    return response.json(), len(chunks)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "590869e8-64de-43a3-8a59-62fe4c5b8699",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for i in range(len(guidelines)):\n",
    "    if i in inserted_guidelines.keys():\n",
    "        continue\n",
    "\n",
    "    res, num_chunks = insert_batch_for_guideline(guidelines[i])\n",
    "    inserted_guidelines[i] = {\n",
    "        \"guideline_awmf_nr\": guidelines[i].awmf_register_number,\n",
    "        \"number_pages\": guidelines[i].download_information.page_count,\n",
    "        \"number_chunks\": num_chunks,\n",
    "        \"missing_chunks\": res[\"failed\"]\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a92c8ea0-3ebf-4165-813c-e3fd720330f3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "inserted_guidelines"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf24818fc5f0b633",
   "metadata": {},
   "source": [
    "## Test out question"
   ]
  },
  {
   "cell_type": "code",
   "id": "e723050f-4c51-4a4c-9cfe-b3985fb7851f",
   "metadata": {},
   "source": [
    "# Get question and system\n",
    "naive_rag_azure_wf = dbi.get_entry(CollectionName.WORKFLOW_SYSTEMS, \"name\", naive_rag_azure_config[\"name\"])\n",
    "if naive_rag_azure_wf is None:\n",
    "    naive_rag_azure_wf_id = init_workflow(BACKEND_API_URL, naive_rag_azure_config)\n",
    "else:\n",
    "    naive_rag_azure_wf_id = dbi.document_to_workflow_system(naive_rag_azure_wf).workflow_id\n",
    "    naive_rag_azure_wf_id = init_workflow_with_id(BACKEND_API_URL, naive_rag_azure_config, naive_rag_azure_wf_id)\n",
    "\n",
    "naive_rag_azure_chat = init_chat(BACKEND_API_URL, naive_rag_azure_wf_id)\n",
    "question = dbi.get_collection(CollectionName.QUESTIONS).find_one().get(\"question\")\n",
    "\n",
    "question"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "03d4734d-9553-41a7-945f-7b711866f8f6",
   "metadata": {},
   "source": [
    "# TEST RETRIEVAL separately\n",
    "response = requests.post(\n",
    "    f\"{BACKEND_API_URL}/knowledge/vector/retriever/search\",\n",
    "    json={\n",
    "        \"class_name\": weaviate_db_config['class_name'],\n",
    "        \"query\": question\n",
    "    }\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "for i, item in enumerate(response.json()['results']):\n",
    "    logger.info(\n",
    "        f\"\"\"\n",
    "## Result {i + 1}\n",
    "  - Guideline: {item['guideline_id']} (chunk {item['chunk_index']})\n",
    "  - Text: '{item['text']}'\n",
    "\"\"\"\n",
    "    )\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b283e323b3ee23cb",
   "metadata": {},
   "source": [
    "answer, retrieval, response_latency = pose_question(BACKEND_API_URL, naive_rag_azure_chat, question)\n",
    "\n",
    "print(f\"### QUESTION: ###\\n{question}\")\n",
    "print(f\"--------------------------------------------------\")\n",
    "print(f\"### ANSWER in {response_latency:.2f} seconds: ###\\n{answer}\")\n",
    "\n",
    "print(\"\\n### RETRIEVAL (Utilized guideline content): ###\")\n",
    "html_table = \"\"\"\n",
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>Index</th>\n",
    "        <th>Guideline ID</th>\n",
    "        <th>Text</th>\n",
    "    </tr>\n",
    "\"\"\"\n",
    "\n",
    "for i, retrieval_entry in enumerate(retrieval):\n",
    "    html_table += f\"\"\"\n",
    "    <tr>\n",
    "        <td>{i}</td>\n",
    "        <td>{retrieval_entry['guideline_id']}</td>\n",
    "        <td>{retrieval_entry['text']}</td>\n",
    "    </tr>\n",
    "\"\"\"\n",
    "html_table += \"</table>\"\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "display(HTML(html_table))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11e4c383aa2475c0",
   "metadata": {},
   "source": [
    "## Creating stored answers"
   ]
  },
  {
   "cell_type": "code",
   "id": "6a27975715612e75",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "naive_rag_azure_wf_system: WorkflowSystem = init_stored_wf_system(dbi, naive_rag_azure_config, BACKEND_API_URL)\n",
    "\n",
    "all_question_docs = list(dbi.get_collection(CollectionName.QUESTIONS).find())\n",
    "skip_ids = [50]\n",
    "logger.progress(\"Progress on answer generation [PROGRESS]: \", 0, len(all_question_docs))\n",
    "processed_ids = [x for x in range(0, 50)]\n",
    "for i, question_doc in enumerate(all_question_docs):\n",
    "    if i in processed_ids:\n",
    "        logger.progress(\"Progress on answer generation [PROGRESS]:\", i + 1, len(all_question_docs))\n",
    "        continue\n",
    "\n",
    "    question: QuestionEntry = dbi.document_to_question_entry(question_doc)\n",
    "    generate_stored_response(dbi, naive_rag_azure_wf_system, None, question, BACKEND_API_URL)\n",
    "    logger.progress(\"Progress on answer generation [PROGRESS]:\", i + 1, len(all_question_docs))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "de780e53555ea6de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gathering feedback\n",
    "For this configuration, we now want to obtain not only the response latency and performance of the retriever, but also two manual evaluation values:\n",
    "1. Correctness\n",
    "2. Hallucination classification\n",
    "3. Generator's factuality score\n",
    "\n",
    "This requires expert responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e01d7996bc154",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63866d7b1a32b590",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Get chats"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4f84681a7b5154d",
   "metadata": {},
   "source": [
    "def get_chats():\n",
    "    naive_rag_azure_config_wf_system: WorkflowSystem = dbi.document_to_workflow_system(\n",
    "        dbi.get_entry(CollectionName.WORKFLOW_SYSTEMS, \"name\", naive_rag_azure_config[\"name\"])\n",
    "    )\n",
    "    chats: List[ChatInteraction] = naive_rag_azure_config_wf_system.generation_results\n",
    "\n",
    "    return chats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "772b74b9205b9ac9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Define functions for widget creation"
   ]
  },
  {
   "cell_type": "code",
   "id": "a31b55c4a0572e31",
   "metadata": {},
   "source": [
    "def create_ui_widgets():\n",
    "    title_html = widgets.HTML()\n",
    "    question_html = widgets.HTML(layout=widgets.Layout(width=\"75%\"))\n",
    "    expected_asw_html = widgets.HTML(layout=widgets.Layout(width=\"75%\"))\n",
    "    provided_asw_html = widgets.HTML(layout=widgets.Layout(width=\"75%\"))\n",
    "    existing_feedback_html = widgets.HTML(layout=widgets.Layout(width=\"75%\"))\n",
    "\n",
    "    feedback_correct_label = widgets.Label(value=\"Correctness:\", layout=widgets.Layout(width='120px'))\n",
    "    feedback_correct_input = widgets.IntSlider(value=3, min=1, max=5, step=1, description=\"\")\n",
    "\n",
    "    feedback_hall_label = widgets.Label(value=\"Hallucination count:\", layout=widgets.Layout(width='120px'))\n",
    "    fc_input = widgets.BoundedIntText(value=0, min=0, max=50, description=\"FC\")\n",
    "    ic_input = widgets.BoundedIntText(value=0, min=0, max=50, description=\"IC\")\n",
    "    cc_input = widgets.BoundedIntText(value=0, min=0, max=50, description=\"CC\")\n",
    "\n",
    "    feedback_factuality_label = widgets.Label(value=\"Count facts for factuality score:\",\n",
    "                                              layout=widgets.Layout(width='120px'))\n",
    "    fs_input = widgets.BoundedIntText(value=0, min=0, max=50, description=\"Supported facts\")\n",
    "    fo_input = widgets.BoundedIntText(value=0, min=0, max=50, description=\"Overall facts\")\n",
    "\n",
    "    feedback_notes_label = widgets.Label(value=\"Note:\", layout=widgets.Layout(width='120px'))\n",
    "    feedback_notes_input = widgets.Textarea(placeholder=\"Enter your name and optional notes...\", description=\"\",\n",
    "                                            disabled=False, layout=widgets.Layout(width=\"75%\", padding=\"0 160px 0 0\"))\n",
    "\n",
    "    return title_html, question_html, expected_asw_html, provided_asw_html, existing_feedback_html, feedback_correct_label, feedback_correct_input, feedback_hall_label, fc_input, ic_input, cc_input, feedback_factuality_label, fs_input, fo_input, feedback_notes_label, feedback_notes_input\n",
    "\n",
    "\n",
    "def create_buttons():\n",
    "    prev_button = widgets.Button(description=\"Previous\")\n",
    "    next_button = widgets.Button(description=\"Next\")\n",
    "    save_button = widgets.Button(description=\"Save\", button_style=\"success\")\n",
    "    exit_button = widgets.Button(description=\"Exit\")\n",
    "\n",
    "    return prev_button, next_button, save_button, exit_button"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "baa0e5e2e25a550c",
   "metadata": {},
   "source": [
    "#### Define parsers for display of chat properties"
   ]
  },
  {
   "cell_type": "code",
   "id": "3b43629f1270f103",
   "metadata": {},
   "source": [
    "def parse_question(question: QuestionEntry, section_title=\"Question\") -> str:\n",
    "    question = f\"\"\"\n",
    "  <details open style=\"margin: 0 20px\">\n",
    "    <summary style='font-weight: bold; font-size: 1.05em; cursor: pointer;'>{section_title}</summary>\n",
    "    <div style='margin: 0px 30px 30px 30px;'>\n",
    "      <span>{html.escape(question.question)}</span><br>\n",
    "      <span><small style='color: gray'>Type: {question.classification.supercategory.value} / {question.classification.subcategory.value}</small></span>\n",
    "    </div>\n",
    "  </details>\n",
    "  \"\"\"\n",
    "    return question\n",
    "\n",
    "\n",
    "def parse_expected_answer_table(ea_list: List[ExpectedAnswer], section_title=\"Expected answer / retrieval\") -> str:\n",
    "    align_style = \"text-align:left; padding: 4px 10px;\"\n",
    "    table = f\"\"\"\n",
    "  <details open style=\"margin: 0 20px\">\n",
    "    <summary style='font-weight: bold; font-size: 1.05em; cursor: pointer;'>{section_title}</summary>\n",
    "    <div style='margin: 0px 30px 30px 30px;'>\n",
    "      <table style='border-collapse: collapse;'>\n",
    "        <thead>\n",
    "          <tr>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>#</th>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>Guideline (AWMF-Nr.)</th>\n",
    "            <th style='{align_style}; border-bottom: 1px solid #ccc;'>Text</th>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>Page in guideline</th>\n",
    "          </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "  \"\"\"\n",
    "    for i, ea in enumerate(ea_list):\n",
    "        table += f\"\"\"\n",
    "          <tr>\n",
    "            <td style='{align_style}'>{i}</td>\n",
    "            <td style='{align_style}'>{ea.guideline.awmf_register_number}</td>\n",
    "            <td style='{align_style}'>{html.escape(ea.text)}</td>\n",
    "            <td style='{align_style}'>{ea.guideline_page}</td>\n",
    "          </tr>\n",
    "    \"\"\"\n",
    "    table += \"\"\"\n",
    "        </tbody>\n",
    "      </table>\n",
    "    </div>\n",
    "  </details>\n",
    "  \"\"\"\n",
    "    return table\n",
    "\n",
    "\n",
    "def parse_provided_answer(pa: GenerationResultEntry, section_title=\"System response (provided answer)\") -> str:\n",
    "    align_style = \"text-align:left; padding: 4px 10px;\"\n",
    "    provided_answer = f\"\"\"\n",
    "  <details open style=\"margin: 0 20px\">\n",
    "    <summary style='font-weight: bold; font-size: 1.05em; cursor: pointer;'>{section_title}</summary>\n",
    "    <div style='margin: 0px 0 30px 30px;'>\n",
    "      {markdown.markdown(pa.answer)}\n",
    "    </div>\n",
    "    <details open style=\"margin: 0 20px\">\n",
    "      <summary style='font-weight: bold; font-size: 1.05em; cursor: pointer;'>Related retrieval</summary>\n",
    "      <div style='margin: 0px 30px 30px 30px;'>\n",
    "        <style>\n",
    "          tr:nth-child(even) {{\n",
    "            background-color: #f2f2f2;\n",
    "          }}\n",
    "        </style>\n",
    "        <table style='border-collapse: collapse;'>\n",
    "          <thead>\n",
    "            <tr>\n",
    "              <th style='{align_style} border-bottom: 1px solid #ccc;'>#</th>\n",
    "              <th style='{align_style} border-bottom: 1px solid #ccc;'>Guideline (AWMF-Nr.)</th>\n",
    "              <th style='{align_style}; border-bottom: 1px solid #ccc;'>Text</th>\n",
    "            </tr>\n",
    "          </thead>\n",
    "          <tbody>\n",
    "  \"\"\"\n",
    "    for i, pa in enumerate(pa.retrieval_result):\n",
    "        provided_answer += f\"\"\"\n",
    "            <tr>\n",
    "              <td style='{align_style}'>{i}</td>\n",
    "              <td style='{align_style}'>{pa.guideline.awmf_register_number}</td>\n",
    "              <td style='{align_style} width:100%;'><details open><summary>...</summary>{html.escape(pa.text)}</details></td>\n",
    "            </tr>\n",
    "    \"\"\"\n",
    "    provided_answer += f\"\"\"\n",
    "          </tbody>\n",
    "        </table>\n",
    "      </div>\n",
    "    </details>\n",
    "  </details>\n",
    "  \"\"\"\n",
    "    return provided_answer\n",
    "\n",
    "\n",
    "def parse_existing_feedback(fb_list: List[Feedback], section_title=\"Already existing evaluation / feedback\") -> str:\n",
    "    if not fb_list:\n",
    "        return \"\"\n",
    "\n",
    "    align_style = \"text-align:left; padding: 4px 10px;\"\n",
    "    table = f\"\"\"\n",
    "  <details open style=\"margin: 0 20px\">\n",
    "    <summary style='font-weight: bold; font-size: 1.05em; cursor: pointer;'>{section_title}</summary>\n",
    "    <div style='margin: 0px 30px 30px 30px;'>\n",
    "      <table style='border-collapse: collapse;'>\n",
    "        <thead>\n",
    "          <tr>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>#</th>\n",
    "            <th style='{align_style}; border-bottom: 1px solid #ccc;'>Target</th>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>Type</th>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>Value</th>\n",
    "            <th style='{align_style} border-bottom: 1px solid #ccc;'>Notes</th>\n",
    "          </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "  \"\"\"\n",
    "    for i, fb in enumerate(fb_list):\n",
    "        table += f\"\"\"\n",
    "          <tr>\n",
    "            <td style='{align_style}'>{i}</td>\n",
    "            <td style='{align_style}'>{fb.target.value}</td>\n",
    "            <td style='{align_style}'>{fb.type.value}</td>\n",
    "            <td style='{align_style}'>{fb.value} <small style='color: gray;'>{'manual' if fb.manual else 'autom.'}</small></td>\n",
    "            <td style='{align_style}'>{fb.notes if fb.notes else '/'}</td>\n",
    "          </tr>\n",
    "    \"\"\"\n",
    "    table += \"\"\"\n",
    "        </tbody>\n",
    "      </table>\n",
    "    </div>\n",
    "  </details>\n",
    "  \"\"\"\n",
    "    return table"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c185bbbe84e1b187",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Actual setup of input display"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f4492dbe82b2ad3",
   "metadata": {},
   "source": [
    "current_idx = 0\n",
    "\n",
    "title_html, question_html, expected_answer_html, provided_answer_html, existing_feedback_html, feedback_correct_label, feedback_correct_input, feedback_hall_label, fc_input, ic_input, cc_input, feedback_factuality_label, fs_input, fo_input, feedback_notes_label, feedback_notes_input = create_ui_widgets()\n",
    "prev_button, next_button, save_button, exit_button = create_buttons()\n",
    "\n",
    "\n",
    "def update(dbi, chat: ChatInteraction, entry: GenerationResultEntry):\n",
    "    correctness_score = feedback_correct_input.value\n",
    "    hallucination_counts = {\n",
    "        \"FC\": fc_input.value,\n",
    "        \"IC\": ic_input.value,\n",
    "        \"CC\": cc_input.value\n",
    "    }\n",
    "    fs, fo = fs_input.value, fo_input.value\n",
    "    note = feedback_notes_input.value\n",
    "\n",
    "    insert_feedback(\n",
    "        dbi, chat, entry, create_feedback_correctness(correctness_score, note)\n",
    "    )\n",
    "    insert_feedback(\n",
    "        dbi, chat, entry, create_feedback_hallucination_classification(hallucination_counts, note)\n",
    "    )\n",
    "    insert_feedback(\n",
    "        dbi, chat, entry, create_feedback_factuality_score(fs, fo, note)\n",
    "    )\n",
    "\n",
    "\n",
    "def show_entry(idx):\n",
    "    global current_idx\n",
    "    current_idx = idx\n",
    "    entry: GenerationResultEntry = chats[idx].entries[0]\n",
    "\n",
    "    title_html.value = f\"\"\"\n",
    "  <div style='display: flex; justify-content: start; align-items: center;'>\n",
    "    <h3 style='margin: 0;'>Chat {idx}</h3>\n",
    "    <small style='margin-left: 10px;'>[{idx + 1}/{len(chats)}]</small>\n",
    "  </div>\n",
    "  \"\"\"\n",
    "    question_html.value = parse_question(entry.question)\n",
    "    expected_answer_html.value = parse_expected_answer_table(entry.question.expected_answers)\n",
    "    provided_answer_html.value = parse_provided_answer(entry)\n",
    "    existing_feedback_html.value = parse_existing_feedback(entry.feedback)\n",
    "\n",
    "    prev_button.disabled = (idx == 0)\n",
    "    next_button.disabled = (idx == len(chats) - 1)\n",
    "    save_button.disabled = (idx == len(chats))\n",
    "\n",
    "\n",
    "def on_prev_clicked(b):\n",
    "    \"\"\"Handle Previous button click: NO SAVING!! show previous entry.\"\"\"\n",
    "    if current_idx > 0:\n",
    "        show_entry(current_idx - 1)\n",
    "\n",
    "\n",
    "def on_next_clicked(b):\n",
    "    \"\"\"Handle Next button click: NO SAVING!! show next entry.\"\"\"\n",
    "    if current_idx < len(chats) - 1:\n",
    "        show_entry(current_idx + 1)\n",
    "\n",
    "\n",
    "def on_save_clicked(b):\n",
    "    \"\"\"Handle Save button click: store current status.\"\"\"\n",
    "    global chats\n",
    "    current_entry = chats[current_idx]\n",
    "    update(dbi, current_entry, current_entry.entries[0])\n",
    "    chats = get_chats()\n",
    "    show_entry(current_idx)\n",
    "    print(f\"stored results for entry {current_idx}\")\n",
    "\n",
    "\n",
    "def on_exit_clicked(b):\n",
    "    \"\"\"Handle Exit button click: NO SAVING!! end the process.\"\"\"\n",
    "    entry_box.layout.display = 'none'\n",
    "    clear_output()\n",
    "    print(\"Finished reviewing entries.\")\n",
    "\n",
    "\n",
    "prev_button.on_click(on_prev_clicked)\n",
    "next_button.on_click(on_next_clicked)\n",
    "save_button.on_click(on_save_clicked)\n",
    "exit_button.on_click(on_exit_clicked)\n",
    "\n",
    "entry_box = widgets.VBox([\n",
    "    title_html,\n",
    "    question_html,\n",
    "    expected_answer_html,\n",
    "    provided_answer_html,\n",
    "    existing_feedback_html,\n",
    "    widgets.HBox([feedback_correct_label, feedback_correct_input], layout=widgets.Layout(margin=\"10px 20px\")),\n",
    "    widgets.HBox([feedback_hall_label, widgets.HBox([fc_input, ic_input, cc_input])], layout=widgets.Layout(margin=\"10px 20px\")),\n",
    "    widgets.HBox([feedback_factuality_label, widgets.HBox([fo_input, fs_input])], layout=widgets.Layout(margin=\"10px 20px\")),\n",
    "    widgets.HBox([feedback_notes_label, feedback_notes_input], layout=widgets.Layout(margin=\"10px 20px\")),\n",
    "    widgets.HBox([prev_button, next_button, save_button, exit_button], layout=widgets.Layout(margin=\"5px 20px\"))\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed5f6d4bd97cc515",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This section requires expert input. Start the next cell, then fill out the provided form.\n",
    "\n",
    "We assume for each chat, that only one interaction is relevant. Meaning, we got one question as an input, and one system response as the output. This means, per question, we got the following information displayed:\n",
    "- Expected answer / retrieval (if available).\n",
    "- Answer provided by the system &rarr; needs evaluation.\n",
    "- Display of current evaluation.\n",
    "\n",
    "The evaluation is focussing on the following aspects:\n",
    "1. Correctness: Check how well response covers the expected information (1 - <small>$0\\%$</small>, 5 - <small>$100\\%$</small>)\n",
    "2. Hallucination classification: Identify and categorize hallucinations &rarr; count for each of these categories:\n",
    "    - FC = fact-conflicting (contradicts world (medical) knowledge)\n",
    "    - IC = input-conflicting (does not address full user input)\n",
    "    - CC = context-conflicting (contradicts provided content like user history or citations)\n",
    "3. Factuality of generator: We want to generate a score that captures how many of the facts in the generated output are supported by retrieved content. For that, the following numbers need to be inserted:\n",
    "    - FO = Overall facts (total number of statements / facts given in generated answer)\n",
    "    - FS = Supported facts (equal or smaller to the total number of facts, as they are these facts BUT only count as supported if the statement is supported by facts given in the retrieval)\n",
    "\n",
    "In addition to the values, you can also add a comment per feedback submission. Ideally, add the following line:\n",
    "- `Evaluator name: ...` with your name inserted\n",
    "\n",
    "For the navigation, keep the following in mind:\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef6b34733e4ef0",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "id": "c22cef6b0cf10dfb",
   "metadata": {},
   "source": [
    "chats: List[ChatInteraction] = get_chats()\n",
    "show_entry(0)\n",
    "entry_box.layout.display = \"\"\n",
    "display(entry_box)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "88912640087f3566",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "This section will now present an overview over the collected feedback and statistics for the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969419782f666a48",
   "metadata": {},
   "source": [
    "### Response latency"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec009bba9e424a7e",
   "metadata": {},
   "source": [
    "naive_rag_azure_wf_system: WorkflowSystem = init_stored_wf_system(dbi, naive_rag_azure_config, BACKEND_API_URL)\n",
    "avg_response_time = get_average_response_time(naive_rag_azure_wf_system)\n",
    "df__response_time, img__response_time = analyze_and_visualize_response_time_per_category(naive_rag_azure_wf_system)\n",
    "img__response_time.update_layout(width=screen_width, height=screen_height)\n",
    "\n",
    "print(f\"Average response latency: {avg_response_time:.2f} s\")\n",
    "for supercat in all_supercategories:\n",
    "    entry = df__response_time[df__response_time['subcategory'].isna() & (df__response_time['supercategory'] == supercat.value)]['avg_response_latency'].iloc[0]\n",
    "    if pd.isna(entry):\n",
    "        print(f\"No response latency for {supercat.value} questions\")\n",
    "    else:\n",
    "        print(f\"Response latency for {supercat.value} questions: {entry:.2f} s\")\n",
    "\n",
    "img__response_time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37c959f24082de8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Correctness of overall system"
   ]
  },
  {
   "cell_type": "code",
   "id": "834d13335973dac2",
   "metadata": {},
   "source": [
    "naive_rag_azure_wf_system: WorkflowSystem = init_stored_wf_system(dbi, naive_rag_azure_config, BACKEND_API_URL)\n",
    "avg_correctness = get_average_correctness(naive_rag_azure_wf_system)\n",
    "df__correctness, img__correctness = analyze_and_visualize_correctness_per_category(naive_rag_azure_wf_system)\n",
    "img__correctness.update_layout(width=screen_width, height=screen_height)\n",
    "\n",
    "print(f\"Average correctness: {avg_correctness:.2f}\")\n",
    "for supercat in all_supercategories:\n",
    "    entry = df__correctness[df__correctness['subcategory'].isna() & (df__correctness['supercategory'] == supercat.value)]['avg_correctness_score'].iloc[0]\n",
    "    if pd.isna(entry):\n",
    "        print(f\"No correctness scores for {supercat.value} questions\")\n",
    "    else:\n",
    "        print(f\"Correctness score for {supercat.value} questions: {entry:.2f}\")\n",
    "\n",
    "img__correctness"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8fc55f1be6b3851b",
   "metadata": {},
   "source": [
    "naive_rag_azure_wf_system: WorkflowSystem = init_stored_wf_system(dbi, naive_rag_azure_config, BACKEND_API_URL)\n",
    "list_hallucinations = get_sum_hallucinations_per_question(naive_rag_azure_wf_system)\n",
    "avg_count_hallucinations_per_question = sum(list_hallucinations) / sum(\n",
    "    1 for item in list_hallucinations if item is not None)\n",
    "df__hallucinations, img__hallucinations = analyze_and_visualize_hallucinations(naive_rag_azure_wf_system)\n",
    "img__hallucinations.update_layout(width=2 * screen_width, height=screen_height)\n",
    "\n",
    "print(f\"Overall hallucinations per question: {avg_count_hallucinations_per_question:.2f}\")\n",
    "\n",
    "img__hallucinations"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f22cedb2b162568",
   "metadata": {},
   "source": [
    "### \"Accuracy\" of system\n",
    "Compare provided answer and expected retrieval with ROUGE, BLEU, METEOR, and embedding similarity."
   ]
  },
  {
   "cell_type": "code",
   "id": "aa7f1a2d0d139666",
   "metadata": {},
   "source": [
    "naive_rag_azure_wf_system: WorkflowSystem = init_stored_wf_system(dbi, naive_rag_azure_config, BACKEND_API_URL)\n",
    "avg_rouge_1, avg_rouge_l, avg_bleu, avg_meteor, avg_similarity = get_average_accuracy_scores(naive_rag_azure_wf_system)\n",
    "df__accuracy, img__accuracy_radar, img__accuracy_box = analyze_and_visualize_accuracy_per_category(\n",
    "    naive_rag_azure_wf_system)\n",
    "img__accuracy_radar.update_layout(width=screen_width, height=screen_height)\n",
    "img__accuracy_box.update_layout(width=2.5 * screen_width, height=screen_height)\n",
    "\n",
    "print(\n",
    "    f\"Average ROUGE-1 [{avg_rouge_1:.2f}], ROUGE-L [{avg_rouge_l:.2f}], BLEU [{avg_bleu:.2f}], METEOR [{avg_meteor:.2f}], embedding similarity [{avg_similarity:.2f}]\")\n",
    "for supercat in all_supercategories:\n",
    "    r1_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_rouge_1_score'].iloc[0]\n",
    "    rl_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_rouge_l_score'].iloc[0]\n",
    "    b_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_bleu_score'].iloc[0]\n",
    "    m_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_meteor_score'].iloc[0]\n",
    "    s_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_embedding_sim_score'].iloc[0]\n",
    "\n",
    "    r1_print = f\"{r1_entry:.2f}\" if not pd.isna(r1_entry) else \"N/A\"\n",
    "    rl_print = f\"{rl_entry:.2f}\" if not pd.isna(rl_entry) else \"N/A\"\n",
    "    b_print = f\"{b_entry:.2f}\" if not pd.isna(b_entry) else \"N/A\"\n",
    "    m_print = f\"{m_entry:.2f}\" if not pd.isna(m_entry) else \"N/A\"\n",
    "    s_print = f\"{s_entry:.2f}\" if not pd.isna(s_entry) else \"N/A\"\n",
    "    print(f\"-> For {supercat.value}: ROUGE-1 [{r1_print}], ROUGE-L [{rl_print}], BLEU [{b_print}], METEOR [{m_print}], embedding similarity [{s_print}]\")\n",
    "\n",
    "img__accuracy_radar.show()\n",
    "img__accuracy_box.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a4031cd15e0e0c74",
   "metadata": {},
   "source": [
    "### Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "322cee0386937e4d",
   "metadata": {},
   "source": [
    "naive_rag_azure_wf_system: WorkflowSystem = init_stored_wf_system(dbi, naive_rag_azure_config, BACKEND_API_URL)\n",
    "avg_precision, avg_recall, avg_f1 = get_average_retrieval_scores(naive_rag_azure_wf_system)\n",
    "df__retrieval, img__precision, img__recall, img__f1 = analyze_and_visualize_retrieval_scores_per_category(\n",
    "    naive_rag_azure_wf_system)\n",
    "img__precision.update_layout(width=screen_width, height=screen_height)\n",
    "img__recall.update_layout(width=screen_width, height=screen_height)\n",
    "img__f1.update_layout(width=screen_width, height=screen_height)\n",
    "\n",
    "print(f\"Average precision: {avg_precision:.2f}, average recall: {avg_recall:.2f}, average F1: {avg_f1:.2f}\")\n",
    "for supercat in all_supercategories:\n",
    "    prec_entry = df__retrieval[df__retrieval['subcategory'].isna() & (df__retrieval['supercategory'] == supercat.value)]['avg_retrieval_precision_score'].iloc[0]\n",
    "    recall_entry = df__retrieval[df__retrieval['subcategory'].isna() & (df__retrieval['supercategory'] == supercat.value)]['avg_retrieval_recall_score'].iloc[0]\n",
    "    f1_entry = df__retrieval[df__retrieval['subcategory'].isna() & (df__retrieval['supercategory'] == supercat.value)]['avg_retrieval_f1_score'].iloc[0]\n",
    "\n",
    "    prec_print = f\"{prec_entry:.2f}\" if not pd.isna(prec_entry) else \"N/A\"\n",
    "    recall_print = f\"{recall_entry:.2f}\" if not pd.isna(recall_entry) else \"N/A\"\n",
    "    f1_print = f\"{f1_entry:.2f}\" if not pd.isna(f1_entry) else \"N/A\"\n",
    "\n",
    "    print(f\"-> For {supercat.value}: precision [{prec_print}], recall [{recall_print}], F1 [{f1_print}]\")\n",
    "\n",
    "img__precision.show()\n",
    "img__recall.show()\n",
    "img__f1.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d8251a973bfc81ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generator's factuality\n",
    "&rarr; utilization of provided retrieval"
   ]
  },
  {
   "cell_type": "code",
   "id": "ff34d9eeef41cdef",
   "metadata": {},
   "source": [
    "naive_rag_azure_wf_system: WorkflowSystem = init_stored_wf_system(dbi, naive_rag_azure_config, BACKEND_API_URL)\n",
    "avg_factuality = get_average_factuality(naive_rag_azure_wf_system)\n",
    "df__factuality, img__factuality = analyze_and_visualize_factuality_per_category(naive_rag_azure_wf_system)\n",
    "img__factuality.update_layout(width=screen_width, height=screen_height)\n",
    "\n",
    "print(f\"Average factuality: {avg_factuality:.2f}\")\n",
    "for supercat in all_supercategories:\n",
    "    entry = df__factuality[df__factuality['subcategory'].isna() & (df__factuality['supercategory'] == supercat.value)]['avg_factuality_score'].iloc[0]\n",
    "    if pd.isna(entry):\n",
    "        print(f\"No factuality scores for {supercat.value} questions\")\n",
    "    else:\n",
    "        print(f\"Factuality score for {supercat.value} questions: {entry:.2f}\")\n",
    "\n",
    "img__factuality"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e0dfd286d4e1f991",
   "metadata": {},
   "source": [
    "### Storing\n",
    "Can alternatively also save the images and numbers."
   ]
  },
  {
   "cell_type": "code",
   "id": "46cb8daa730f52f1",
   "metadata": {},
   "source": [
    "os.makedirs(os.path.dirname(statistics_doc), exist_ok=True)\n",
    "with open(statistics_doc, \"a\", encoding=\"utf-8\") as statistics_file:\n",
    "    # Response latency\n",
    "    statistics_file.write(f\"\\n\\nAverage response latency across all questions: {avg_response_time:.2f} s\\n\")\n",
    "    for supercat in all_supercategories:\n",
    "        entry = df__response_time[df__response_time['subcategory'].isna() & (df__response_time['supercategory'] == supercat.value)]['avg_response_latency']\n",
    "        if len(entry) == 0:\n",
    "            statistics_file.write(f\"No response latency for {supercat.value} questions\\n\")\n",
    "        else:\n",
    "            statistics_file.write(f\"Response latency for {supercat.value} questions: {entry.iloc[0]:.2f} s\\n\")\n",
    "\n",
    "img__response_time.write_image(response_time_img, width=width, height=height)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac1f087948c93032",
   "metadata": {},
   "source": [
    "os.makedirs(os.path.dirname(statistics_doc), exist_ok=True)\n",
    "with open(statistics_doc, \"a\", encoding=\"utf-8\") as statistics_file:\n",
    "    # Correctness score\n",
    "    statistics_file.write(f\"\\n\\nAverage correctness score across all questions: {avg_correctness:.2f}\\n\")\n",
    "    for supercat in all_supercategories:\n",
    "        entry = df__correctness[\n",
    "            df__correctness['subcategory'].isna() &\n",
    "            (df__correctness['supercategory'] == supercat.value)\n",
    "            ]['avg_correctness_score']\n",
    "        if entry.empty or pd.isna(entry.iloc[0]):\n",
    "            statistics_file.write(f\"No correctness scores for {supercat.value} questions\\n\")\n",
    "        else:\n",
    "            statistics_file.write(f\"Correctness score for {supercat.value} questions: {entry.iloc[0]:.2f}\\n\")\n",
    "\n",
    "img__correctness.write_image(correctness_score_img, width=width, height=height)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ee4cedd61dc49fd",
   "metadata": {},
   "source": [
    "os.makedirs(os.path.dirname(statistics_doc), exist_ok=True)\n",
    "with open(statistics_doc, \"a\", encoding=\"utf-8\") as statistics_file:\n",
    "    # Hallucinations\n",
    "    statistics_file.write(\n",
    "        f\"\\n\\nAverage hallucination count per question: {avg_count_hallucinations_per_question:.2f}\\n\")\n",
    "\n",
    "img__hallucinations.write_image(hallucinations_img, width=2.5 * width, height=height)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8c0c238dd39447",
   "metadata": {},
   "source": [
    "os.makedirs(os.path.dirname(statistics_doc), exist_ok=True)\n",
    "with open(statistics_doc, \"a\", encoding=\"utf-8\") as statistics_file:\n",
    "    # \"Accuracy\"\n",
    "    statistics_file.write(\n",
    "        f\"\\n\\nAverage ROUGE-1 [{avg_rouge_1:.2f}], ROUGE-L [{avg_rouge_l:.2f}], BLEU [{avg_bleu:.2f}], METEOR [{avg_meteor:.2f}], embedding similarity [{avg_similarity:.2f}]\\n\")\n",
    "    for supercat in all_supercategories:\n",
    "        r1_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_rouge_1_score'].iloc[0]\n",
    "        rl_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_rouge_l_score'].iloc[0]\n",
    "        b_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_bleu_score'].iloc[0]\n",
    "        m_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_meteor_score'].iloc[0]\n",
    "        s_entry = df__accuracy[df__accuracy['subcategory'].isna() & (df__accuracy['supercategory'] == supercat.value)]['avg_embedding_sim_score'].iloc[0]\n",
    "\n",
    "        r1_print = f\"{r1_entry:.2f}\" if not pd.isna(r1_entry) else \"N/A\"\n",
    "        rl_print = f\"{rl_entry:.2f}\" if not pd.isna(rl_entry) else \"N/A\"\n",
    "        b_print = f\"{b_entry:.2f}\" if not pd.isna(b_entry) else \"N/A\"\n",
    "        m_print = f\"{m_entry:.2f}\" if not pd.isna(m_entry) else \"N/A\"\n",
    "        s_print = f\"{s_entry:.2f}\" if not pd.isna(s_entry) else \"N/A\"\n",
    "\n",
    "        statistics_file.write(\n",
    "            f\"-> For {supercat.value}: ROUGE-1 [{r1_print}], ROUGE-L [{rl_print}], BLEU [{b_print}], METEOR [{m_print}], embedding similarity [{s_print}]\\n\")\n",
    "\n",
    "img__accuracy_radar.write_image(accuracy_radar_img, width=width, height=height)\n",
    "img__accuracy_box.write_image(accuracy_box_img, width=2.5 * width, height=height)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4dd75df2417a3825",
   "metadata": {},
   "source": [
    "os.makedirs(os.path.dirname(statistics_doc), exist_ok=True)\n",
    "with open(statistics_doc, \"a\", encoding=\"utf-8\") as statistics_file:\n",
    "    # Retriever quality\n",
    "    statistics_file.write(\n",
    "        f\"\\n\\nAverage precision: {avg_precision:.2f}, average recall: {avg_recall:.2f}, average F1: {avg_f1:.2f}\\n\")\n",
    "    for supercat in all_supercategories:\n",
    "        prec_entry = df__retrieval[df__retrieval['subcategory'].isna() & (df__retrieval['supercategory'] == supercat.value)]['avg_retrieval_precision_score'].iloc[0]\n",
    "        recall_entry = df__retrieval[df__retrieval['subcategory'].isna() & (df__retrieval['supercategory'] == supercat.value)]['avg_retrieval_recall_score'].iloc[0]\n",
    "        f1_entry = df__retrieval[df__retrieval['subcategory'].isna() & (df__retrieval['supercategory'] == supercat.value)]['avg_retrieval_f1_score'].iloc[0]\n",
    "\n",
    "        prec_print = f\"{prec_entry:.2f}\" if not pd.isna(prec_entry) else \"N/A\"\n",
    "        recall_print = f\"{recall_entry:.2f}\" if not pd.isna(recall_entry) else \"N/A\"\n",
    "        f1_print = f\"{f1_entry:.2f}\" if not pd.isna(f1_entry) else \"N/A\"\n",
    "\n",
    "        statistics_file.write(\n",
    "            f\"-> For {supercat.value}: precision [{prec_print}], recall [{recall_print}], F1 [{f1_print}]\\n\")\n",
    "\n",
    "img__precision.write_image(retrieval_precision_img, width=width, height=height)\n",
    "img__recall.write_image(retrieval_recall_img, width=width, height=height)\n",
    "img__f1.write_image(retrieval_f1_img, width=width, height=height)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf2c3f7a7e9e0d7",
   "metadata": {},
   "source": [
    "os.makedirs(os.path.dirname(statistics_doc), exist_ok=True)\n",
    "with open(statistics_doc, \"a\", encoding=\"utf-8\") as statistics_file:\n",
    "    # Generator quality\n",
    "    statistics_file.write(f\"Average factuality: {avg_factuality:.2f}\")\n",
    "    for supercat in all_supercategories:\n",
    "        entry = df__factuality[df__factuality['subcategory'].isna() & (df__factuality['supercategory'] == supercat.value)]['avg_factuality_score'].iloc[0]\n",
    "        if pd.isna(entry):\n",
    "            statistics_file.write(f\"No factuality scores for {supercat.value} questions\")\n",
    "        else:\n",
    "            statistics_file.write(f\"Factuality score for {supercat.value} questions: {entry:.2f}\")\n",
    "\n",
    "img__factuality.write_image(retrieval_factuality_img, width=width, height=height)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc296c4212fa6a53",
   "metadata": {},
   "source": [
    "## Shutdown"
   ]
  },
  {
   "cell_type": "code",
   "id": "4fdce16877a6a111",
   "metadata": {},
   "source": [
    "dbi.close()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
