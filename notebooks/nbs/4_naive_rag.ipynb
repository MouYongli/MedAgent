{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60525002ddf614db",
   "metadata": {},
   "source": [
    "# Demo for MedAgent - First answer generation with naive RAG pipeline\n",
    "\n",
    "This is the manual testing playground to test some basic workflows later properly implemented in the MedAgent repository.\n",
    "\n",
    "This file is responsible for a first test of answer generation with naive retrieval (basically creating the second baseline for our system test). This means, for the question first the most similar chunks from the guidelines are retrieved, and then provided to a generator with the original question. For this setup, new feedback must be gathered and the results analyzed and visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30139ef9da2d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "from general.helper.mongodb_interactor import MongoDBInterface, CollectionName\n",
    "from general.helper.embedder import OpenAIEmbedder\n",
    "from general.helper.logging import logger\n",
    "from scripts.Guideline.guideline_interaction import get_plain_text_from_pdf\n",
    "from scripts.System.system_setup import load_system_json\n",
    "from scripts.System.system_interaction import init_workflow, init_workflow_with_id, init_chat, pose_question\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.local-env\")\n",
    "BACKEND_API_URL = \"http://host.docker.internal:5000/api\"\n",
    "mongo_url = os.getenv(\"MONGO_URL\", \"mongodb://mongo:mongo@host.docker.internal:27017/\")\n",
    "\n",
    "weaviate_db_config = load_system_json(\"./input/database_setups/weaviate_custom_vectorizer.json\")\n",
    "naive_rag_azure_config = load_system_json(\"./input/system/naive_rag_azure.json\")\n",
    "text_output_dir = \"output/guideline/plain_text/\"\n",
    "for file_or_dir in [text_output_dir]:\n",
    "    os.makedirs(os.path.dirname(file_or_dir), exist_ok=True)\n",
    "\n",
    "dbi = MongoDBInterface(mongo_url)\n",
    "dbi.register_collections(\n",
    "    CollectionName.GUIDELINES,\n",
    "    CollectionName.WORKFLOW_SYSTEMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5e9e53491be494",
   "metadata": {},
   "source": [
    "## Setup vector database\n",
    "In the first jupyter notebook, the guideline were already downloaded and stored in a MongoDB. To now be utilizable for the naive RAG flow, their content now needs to be cut up and stored in a vector database (for now Milvus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc3dd3a-592f-4055-b947-e2207be757af",
   "metadata": {},
   "outputs": [],
   "source": [
    "guideline_documents = list(dbi.get_collection(CollectionName.GUIDELINES).find())\n",
    "guidelines = [\n",
    "    dbi.document_to_guideline_metadata(doc) for doc in guideline_documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9af2ad42-fc5f-4dfc-aef4-06ee9a1b04ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m2025-04-16 08:07:31\u001b[0m \u001b[37m[\u001b[0m\u001b[1m\u001b[38;5;208mINFO\u001b[0m\u001b[37m]\u001b[0m \u001b[38;5;208mResult of deletion for GuidelineChunksCustomVector: <Response [200]>\u001b[0m\n",
      "\u001b[37m2025-04-16 08:07:32\u001b[0m \u001b[37m[\u001b[0m\u001b[1m\u001b[38;5;208mINFO\u001b[0m\u001b[37m]\u001b[0m \u001b[38;5;208m<Response [200]>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# comment out if not want to overwrite\n",
    "response = requests.delete(f\"{BACKEND_API_URL}/knowledge/vector/retriever/delete/{weaviate_db_config['class_name']}\")\n",
    "logger.info(f\"Result of deletion for {weaviate_db_config['class_name']}: {response}\")\n",
    "\n",
    "response = requests.post(f\"{BACKEND_API_URL}/knowledge/vector/retriever/init\", json=weaviate_db_config)\n",
    "try:\n",
    "    response.raise_for_status()\n",
    "    logger.info(response)\n",
    "except Exception as e:\n",
    "    detail = response.json().get(\"detail\", \"\")\n",
    "    if \"already exists\" in detail:\n",
    "        logger.info(f\"Weaviate collection already exists: {detail}\")\n",
    "    else:\n",
    "        logger.error(f\"Failed to initialize Weaviate collection: {detail}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f4146a859e6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedder = OpenAIEmbedder(\n",
    "#    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "#    api_base=os.getenv(\"AZURE_OPENAI_API_BASE\"),\n",
    "#    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "#    deployment_name=\"text-embedding-3-small\" # or later: text-embedding-3-small\n",
    "#)\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 512) -> List[str]:\n",
    "    words = text.split()\n",
    "    chunks, current = [], []\n",
    "    token_count = lambda x: len(encoding.encode(\" \".join(x)))\n",
    "\n",
    "    for word in words:\n",
    "        current.append(word)\n",
    "        if token_count(current) >= max_tokens:\n",
    "            chunks.append(\" \".join(current[:-1]))\n",
    "            current = [word]\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca67ee-d57f-4cca-83e1-7720259f0ccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Guidelines [PROGRESS]: : [                                                  ] 0% (0/93)\u001b[0m"
     ]
    }
   ],
   "source": [
    "logger.progress(\"Processing Guidelines [PROGRESS]: \", 0, len(guidelines))\n",
    "for i_g, g in enumerate(guidelines):\n",
    "    try:\n",
    "        text = get_plain_text_from_pdf(g.download_information.file_path, text_output_dir)\n",
    "        chunks = chunk_text(text)\n",
    "        if chunks == []:\n",
    "            logger.error(f\"[{g.awmf_register_numner}] Something went wrong with reading the text or chunking -> empty\")\n",
    "        for i_c, chunk in enumerate(chunks):\n",
    "            try:\n",
    "                #vector = embedder.embed(chunk)\n",
    "                insert_entity = {\n",
    "                    \"text\": chunk,\n",
    "                    #\"vector\": vector,\n",
    "                    \"metadata\": {\n",
    "                        \"guideline_id\": g.awmf_register_number,\n",
    "                        \"chunk_index\": i_c\n",
    "                    },\n",
    "                    \"class_name\": weaviate_db_config['class_name']\n",
    "                }\n",
    "                #logger.info(insert_entity)\n",
    "                response = requests.post(\n",
    "                    f\"{BACKEND_API_URL}/knowledge/vector/retriever/insert\",\n",
    "                    json = insert_entity\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "            except Exception as chunk_error:\n",
    "                logger.error(f\"[{g.awmf_register_number}] Failed to process chunk {i_c}: {chunk_error}\")\n",
    "        logger.progress(\"Processing Guidelines [PROGRESS]: \", i_g+1, len(guidelines))\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[{g.awmf_register_number}] Failed to process guideline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf24818fc5f0b633",
   "metadata": {},
   "source": [
    "## Test out question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b283e323b3ee23cb",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### QUESTION: ###\n",
      "Wann ist die dreidimensionale Bildgebung bei der Entfernung von Weisheitszähnen indiziert?\n",
      "--------------------------------------------------\n",
      "### ANSWER in 3.72 seconds: ###\n",
      "Die dreidimensionale Bildgebung, wie beispielsweise die digitale Volumentomographie (DVT), ist bei der Entfernung von Weisheitszähnen in folgenden Fällen indiziert:\n",
      "\n",
      "1. **Komplexe anatomische Verhältnisse**: Wenn die Weisheitszähne in einer Position liegen, die nahe an wichtigen anatomischen Strukturen wie Nerven oder Kieferhöhlen sind. Dies ermöglicht eine genauere Planung der chirurgischen Intervention.\n",
      "\n",
      "2. **Vorhandensein von Zysten oder Tumoren**: Bei Verdacht auf zystische oder tumoröse Veränderungen im Bereich der Weisheitszähne sollte eine dreidimensionale Bildgebung in Betracht gezogen werden, um die Ausdehnung und die Beziehung zu umgebenden Strukturen besser zu verstehen.\n",
      "\n",
      "3. **Unklare Röntgenbefunde**: Wenn die zweidimensionalen Röntgenaufnahmen (z. B. Panoramaschichtaufnahmen) keine ausreichenden Informationen über die Wurzelanatomie oder die Position der Zähne liefern, kann 3D-Bildgebung hilfreich sein.\n",
      "\n",
      "4. **Vorbereitung auf komplexe chirurgische Eingriffe**: Bei geplanten operativen Eingriffen, die besondere Präzision erfordern, kann die dreidimensionale Bildgebung die Planung und Durchführung erleichtern.\n",
      "\n",
      "Insgesamt bietet die dreidimensionale Bildgebung eine bessere Visualisierung der Kieferanatomie und kann dazu beitragen, postoperative Komplikationen zu vermeiden.\n"
     ]
    }
   ],
   "source": [
    "naive_rag_azure_wf = dbi.get_entry(CollectionName.WORKFLOW_SYSTEMS, \"name\", naive_rag_azure_config[\"name\"])\n",
    "if naive_rag_azure_wf is None:\n",
    "    naive_rag_azure_wf_id = init_workflow(BACKEND_API_URL, naive_rag_azure_config)\n",
    "else:\n",
    "    naive_rag_azure_wf_id = dbi.document_to_workflow_system(naive_rag_azure_wf).workflow_id\n",
    "    naive_rag_azure_wf_id = init_workflow_with_id(BACKEND_API_URL, naive_rag_azure_config, naive_rag_azure_wf_id)\n",
    "\n",
    "naive_rag_azure_chat = init_chat(BACKEND_API_URL, naive_rag_azure_wf_id)\n",
    "question = dbi.get_collection(CollectionName.QUESTIONS).find_one().get(\"question\")\n",
    "answer, response_latency = pose_question(BACKEND_API_URL, naive_rag_azure_chat, question)\n",
    "\n",
    "print(f\"### QUESTION: ###\\n{question}\")\n",
    "print(f\"--------------------------------------------------\")\n",
    "print(f\"### ANSWER in {response_latency:.2f} seconds: ###\\n{answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
